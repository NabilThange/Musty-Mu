---
description: 
globs: 
alwaysApply: false
---
##ABOUT GROQ 
# Comprehensive GROQ Integration Overview for FlowGenie Chat Application

## 1. What is GROQ and Its Purpose in the Project

### Understanding GROQ

**GROQ** (Graph Relational Object Queries) in this context refers to **Groq Inc.'s AI inference API**, not the query language. Groq is a company that provides ultra-fast AI inference through their specialized hardware called Language Processing Units (LPUs).

### Key Characteristics of Groq:

- **Speed**: Delivers extremely fast token generation (up to 500+ tokens/second)
- **Cost-Effective**: Competitive pricing compared to other AI providers
- **OpenAI-Compatible API**: Uses the same API format as OpenAI, making integration seamless
- **Multiple Models**: Supports various open-source models like Llama, Mixtral, and Gemma


### Purpose in FlowGenie Project:

```typescript
// The project uses Groq to power the AI chat functionality
// providing users with fast, intelligent responses to their queries
```

**Primary Functions:**

1. **Real-time Chat Responses**: Generate intelligent replies to user messages
2. **Auto-Title Generation**: Create meaningful titles for chat threads
3. **Content Processing**: Handle various AI-powered features within the chat interface


---

## 2. Specific Coding Examples in the Integration

### 2.1 Main Chat API Route (`app/api/chat/ask/route.ts`)

```typescript
import { type NextRequest, NextResponse } from "next/server"

export async function POST(req: NextRequest) {
  try {
    // Extract parameters from the request
    const { messages, temperature = 0.7, model = "llama3-70b-8192" } = await req.json()

    // Make API call to Groq's OpenAI-compatible endpoint
    const res = await fetch("https://api.groq.com/openai/v1/chat/completions", {
      method: "POST",
      headers: {
        Authorization: `Bearer ${process.env.GROQ_API_KEY}`, // Secure API key
        "Content-Type": "application/json",
      },
      body: JSON.stringify({
        messages,      // Array of conversation messages
        temperature,   // Controls randomness (0.0 = deterministic, 1.0 = creative)
        model,        // Specific Groq model to use
      }),
    })

    // Handle API response
    if (!res.ok) {
      const errorData = await res.json()
      console.error("Groq API error:", errorData)
      return NextResponse.json(
        { error: "Failed to get response from Groq API", details: errorData },
        { status: res.status },
      )
    }

    // Extract and return the AI response
    const data = await res.json()
    const content = data?.choices?.[0]?.message?.content

    return NextResponse.json({ content })
  } catch (error) {
    console.error("Error in chat API route:", error)
    return NextResponse.json({ error: "An unexpected error occurred" }, { status: 500 })
  }
}
```

### 2.2 Title Generation API (`app/api/chat/generate-title/route.ts`)

```typescript
import { NextRequest, NextResponse } from "next/server"

export async function POST(req: NextRequest) {
  try {
    const { content } = await req.json()

    // Specialized prompt for generating concise chat titles
    const response = await fetch("https://api.groq.com/openai/v1/chat/completions", {
      method: "POST",
      headers: {
        Authorization: `Bearer ${process.env.GROQ_API_KEY}`,
        "Content-Type": "application/json",
      },
      body: JSON.stringify({
        model: "llama3-70b-8192",
        messages: [
          {
            role: "system",
            content: "Generate a concise, descriptive title (max 6 words) for this conversation. Return only the title, no quotes or extra text."
          },
          {
            role: "user", 
            content: `Generate a title for this conversation: ${content}`
          }
        ],
        temperature: 0.3, // Lower temperature for more consistent titles
        max_tokens: 20,   // Limit response length
      }),
    })

    const data = await response.json()
    const title = data?.choices?.[0]?.message?.content?.trim() || "New Chat"

    return NextResponse.json({ title })
  } catch (error) {
    console.error("Error generating title:", error)
    return NextResponse.json({ title: "New Chat" }, { status: 500 })
  }
}
```

### 2.3 Frontend Integration (`hooks/use-chat.ts`)

```typescript
// Custom hook that manages chat state and Groq API calls
export function useChat() {
  const [messages, setMessages] = useState<Message[]>([])
  const [isLoading, setIsLoading] = useState(false)

  const sendMessage = async (content: string) => {
    try {
      setIsLoading(true)
      
      // Add user message to state
      const userMessage: Message = {
        id: generateId(),
        content,
        role: "user",
        timestamp: new Date(),
      }
      setMessages(prev => [...prev, userMessage])

      // Prepare conversation history for Groq
      const conversationHistory = [...messages, userMessage].map(msg => ({
        role: msg.role,
        content: msg.content
      }))

      // Call our API route which connects to Groq
      const response = await fetch("/api/chat/ask", {
        method: "POST",
        headers: { "Content-Type": "application/json" },
        body: JSON.stringify({
          messages: conversationHistory,
          temperature: 0.7,
          model: "llama3-70b-8192"
        }),
      })

      const data = await response.json()
      
      // Add AI response to state
      const assistantMessage: Message = {
        id: generateId(),
        content: data.content,
        role: "assistant", 
        timestamp: new Date(),
      }
      setMessages(prev => [...prev, assistantMessage])

    } catch (error) {
      console.error("Error sending message:", error)
    } finally {
      setIsLoading(false)
    }
  }

  return { messages, sendMessage, isLoading }
}
```

---

## 3. Logic and Structure of Groq Integration

### 3.1 Message Flow Architecture

```mermaid
Diagram.download-icon {
            cursor: pointer;
            transform-origin: center;
        }
        .download-icon .arrow-part {
            transition: transform 0.35s cubic-bezier(0.35, 0.2, 0.14, 0.95);
             transform-origin: center;
        }
        button:has(.download-icon):hover .download-icon .arrow-part, button:has(.download-icon):focus-visible .download-icon .arrow-part {
          transform: translateY(-1.5px);
        }
        #mermaid-diagram-r1pm{font-family:var(--font-geist-sans);font-size:12px;fill:#000000;}#mermaid-diagram-r1pm .error-icon{fill:#552222;}#mermaid-diagram-r1pm .error-text{fill:#552222;stroke:#552222;}#mermaid-diagram-r1pm .edge-thickness-normal{stroke-width:1px;}#mermaid-diagram-r1pm .edge-thickness-thick{stroke-width:3.5px;}#mermaid-diagram-r1pm .edge-pattern-solid{stroke-dasharray:0;}#mermaid-diagram-r1pm .edge-thickness-invisible{stroke-width:0;fill:none;}#mermaid-diagram-r1pm .edge-pattern-dashed{stroke-dasharray:3;}#mermaid-diagram-r1pm .edge-pattern-dotted{stroke-dasharray:2;}#mermaid-diagram-r1pm .marker{fill:#666;stroke:#666;}#mermaid-diagram-r1pm .marker.cross{stroke:#666;}#mermaid-diagram-r1pm svg{font-family:var(--font-geist-sans);font-size:12px;}#mermaid-diagram-r1pm p{margin:0;}#mermaid-diagram-r1pm .label{font-family:var(--font-geist-sans);color:#000000;}#mermaid-diagram-r1pm .cluster-label text{fill:#333;}#mermaid-diagram-r1pm .cluster-label span{color:#333;}#mermaid-diagram-r1pm .cluster-label span p{background-color:transparent;}#mermaid-diagram-r1pm .label text,#mermaid-diagram-r1pm span{fill:#000000;color:#000000;}#mermaid-diagram-r1pm .node rect,#mermaid-diagram-r1pm .node circle,#mermaid-diagram-r1pm .node ellipse,#mermaid-diagram-r1pm .node polygon,#mermaid-diagram-r1pm .node path{fill:#eee;stroke:#999;stroke-width:1px;}#mermaid-diagram-r1pm .rough-node .label text,#mermaid-diagram-r1pm .node .label text{text-anchor:middle;}#mermaid-diagram-r1pm .node .katex path{fill:#000;stroke:#000;stroke-width:1px;}#mermaid-diagram-r1pm .node .label{text-align:center;}#mermaid-diagram-r1pm .node.clickable{cursor:pointer;}#mermaid-diagram-r1pm .arrowheadPath{fill:#333333;}#mermaid-diagram-r1pm .edgePath .path{stroke:#666;stroke-width:2.0px;}#mermaid-diagram-r1pm .flowchart-link{stroke:#666;fill:none;}#mermaid-diagram-r1pm .edgeLabel{background-color:white;text-align:center;}#mermaid-diagram-r1pm .edgeLabel p{background-color:white;}#mermaid-diagram-r1pm .edgeLabel rect{opacity:0.5;background-color:white;fill:white;}#mermaid-diagram-r1pm .labelBkg{background-color:rgba(255, 255, 255, 0.5);}#mermaid-diagram-r1pm .cluster rect{fill:hsl(0, 0%, 98.9215686275%);stroke:#707070;stroke-width:1px;}#mermaid-diagram-r1pm .cluster text{fill:#333;}#mermaid-diagram-r1pm .cluster span{color:#333;}#mermaid-diagram-r1pm div.mermaidTooltip{position:absolute;text-align:center;max-width:200px;padding:2px;font-family:var(--font-geist-sans);font-size:12px;background:hsl(-160, 0%, 93.3333333333%);border:1px solid #707070;border-radius:2px;pointer-events:none;z-index:100;}#mermaid-diagram-r1pm .flowchartTitleText{text-anchor:middle;font-size:18px;fill:#000000;}#mermaid-diagram-r1pm .flowchart-link{stroke:hsl(var(--gray-400));stroke-width:1px;}#mermaid-diagram-r1pm .marker,#mermaid-diagram-r1pm marker,#mermaid-diagram-r1pm marker *{fill:hsl(var(--gray-400))!important;stroke:hsl(var(--gray-400))!important;}#mermaid-diagram-r1pm .label,#mermaid-diagram-r1pm text,#mermaid-diagram-r1pm text>tspan{fill:hsl(var(--black))!important;color:hsl(var(--black))!important;}#mermaid-diagram-r1pm .background,#mermaid-diagram-r1pm rect.relationshipLabelBox{fill:hsl(var(--white))!important;}#mermaid-diagram-r1pm .entityBox,#mermaid-diagram-r1pm .attributeBoxEven{fill:hsl(var(--gray-150))!important;}#mermaid-diagram-r1pm .attributeBoxOdd{fill:hsl(var(--white))!important;}#mermaid-diagram-r1pm .label-container,#mermaid-diagram-r1pm rect.actor{fill:hsl(var(--white))!important;stroke:hsl(var(--gray-400))!important;}#mermaid-diagram-r1pm line{stroke:hsl(var(--gray-400))!important;}#mermaid-diagram-r1pm :root{--mermaid-font-family:var(--font-geist-sans);}User Types MessageFrontend useChat HookAdd User Message to StatePrepare Conversation HistoryCall /api/chat/askNext.js API RouteGroq API CallProcess ResponseReturn to FrontendAdd AI Response to StateUpdate UI
```

### 3.2 Data Structure Patterns

**Message Format:**

```typescript
interface Message {
  id: string
  content: string
  role: "user" | "assistant" | "system"
  timestamp: Date
  isTyping?: boolean
  files?: MessageFile[]
}
```

**Groq API Request Structure:**

```typescript
interface GroqRequest {
  model: string              // e.g., "llama3-70b-8192"
  messages: Array<{
    role: "user" | "assistant" | "system"
    content: string
  }>
  temperature: number        // 0.0 to 1.0
  max_tokens?: number       // Optional response length limit
  stream?: boolean          // For real-time streaming responses
}
```

### 3.3 Error Handling Strategy

```typescript
// Multi-layer error handling approach
try {
  // 1. Network/API errors
  const response = await fetch(groqEndpoint, options)
  
  if (!response.ok) {
    // 2. HTTP status errors
    throw new Error(`HTTP ${response.status}: ${response.statusText}`)
  }
  
  const data = await response.json()
  
  // 3. Response validation
  if (!data?.choices?.[0]?.message?.content) {
    throw new Error("Invalid response format from Groq API")
  }
  
  return data.choices[0].message.content
  
} catch (error) {
  // 4. Graceful degradation
  console.error("Groq API error:", error)
  return "I'm sorry, I'm having trouble responding right now. Please try again."
}
```

---

## 4. Component Interactions and Integration Points

### 4.1 Authentication Layer Integration

```typescript
// ChatService integrates Groq with Supabase authentication
export class ChatService {
  static async addMessage(threadId: string, userId: string, role: string, content: string) {
    // 1. Store user message in Supabase
    const userMessage = await supabase.from("chat_messages").insert({
      thread_id: threadId,
      user_id: userId,
      role: "user",
      content: content
    })

    // 2. Get AI response from Groq
    const aiResponse = await fetch("/api/chat/ask", {
      method: "POST",
      body: JSON.stringify({ messages: conversationHistory })
    })

    // 3. Store AI response in Supabase
    const assistantMessage = await supabase.from("chat_messages").insert({
      thread_id: threadId,
      user_id: userId,
      role: "assistant", 
      content: aiResponse.content,
      model_used: "llama3-70b-8192",
      tokens_used: aiResponse.usage?.total_tokens
    })
  }
}
```

### 4.2 Real-time UI Updates

```typescript
// Chat component manages real-time interaction with Groq
export function ChatWindow() {
  const { messages, sendMessage, isLoading } = useChat()
  
  const handleSendMessage = async (content: string) => {
    // 1. Immediate UI update (optimistic)
    setMessages(prev => [...prev, { 
      id: tempId, 
      content, 
      role: "user", 
      timestamp: new Date() 
    }])
    
    // 2. Show typing indicator
    setMessages(prev => [...prev, {
      id: "typing",
      content: "",
      role: "assistant",
      isTyping: true,
      timestamp: new Date()
    }])
    
    // 3. Get Groq response
    const response = await sendMessage(content)
    
    // 4. Replace typing indicator with actual response
    setMessages(prev => prev.map(msg => 
      msg.id === "typing" 
        ? { ...msg, content: response, isTyping: false, id: generateId() }
        : msg
    ))
  }
  
  return (
    <div className="chat-window">
      {messages.map(message => (
        <MessageComponent 
          key={message.id} 
          message={message}
          isTyping={message.isTyping}
        />
      ))}
      <ChatInput onSend={handleSendMessage} disabled={isLoading} />
    </div>
  )
}
```

### 4.3 Database Integration Flow

```typescript
// Complete flow from user input to database storage
const completeMessageFlow = async (userInput: string, threadId: string) => {
  // 1. Save user message to database
  const userMessage = await ChatService.addMessage(
    threadId, 
    userId, 
    "user", 
    userInput
  )
  
  // 2. Get conversation history from database
  const { data: messageHistory } = await ChatService.getThreadMessages(threadId)
  
  // 3. Format for Groq API
  const groqMessages = messageHistory.map(msg => ({
    role: msg.role,
    content: msg.content
  }))
  
  // 4. Call Groq API
  const startTime = Date.now()
  const aiResponse = await fetch("/api/chat/ask", {
    method: "POST",
    body: JSON.stringify({ messages: groqMessages })
  })
  const processingTime = Date.now() - startTime
  
  // 5. Save AI response with metadata
  await ChatService.addMessage(
    threadId,
    userId,
    "assistant", 
    aiResponse.content,
    {
      model_used: "llama3-70b-8192",
      processing_time_ms: processingTime,
      tokens_used: aiResponse.usage?.total_tokens
    }
  )
  
  // 6. Auto-generate thread title if first message
  if (messageHistory.length === 1) {
    await ChatService.generateThreadTitle(threadId)
  }
}
```

---

## 5. Performance and Optimization Considerations

### 5.1 Response Speed Optimization

```typescript
// Groq's strength: Ultra-fast inference
const optimizedGroqCall = async (messages: Message[]) => {
  const startTime = performance.now()
  
  const response = await fetch("https://api.groq.com/openai/v1/chat/completions", {
    method: "POST",
    headers: {
      Authorization: `Bearer ${process.env.GROQ_API_KEY}`,
      "Content-Type": "application/json",
    },
    body: JSON.stringify({
      model: "llama3-70b-8192", // Groq's fastest model
      messages,
      temperature: 0.7,
      max_tokens: 1000,        // Limit for faster responses
      stream: false            // Set to true for streaming responses
    }),
  })
  
  const endTime = performance.now()
  console.log(`Groq response time: ${endTime - startTime}ms`) // Typically 200-500ms
  
  return response
}
```

### 5.2 Cost Management

```typescript
// Token usage tracking for cost optimization
const trackTokenUsage = async (messages: Message[]) => {
  const response = await groqApiCall(messages)
  
  // Log usage for monitoring
  console.log({
    model: "llama3-70b-8192",
    promptTokens: response.usage.prompt_tokens,
    completionTokens: response.usage.completion_tokens,
    totalTokens: response.usage.total_tokens,
    estimatedCost: calculateCost(response.usage.total_tokens)
  })
  
  // Store in database for analytics
  await ChatService.updateMessage(messageId, {
    tokens_used: response.usage.total_tokens,
    model_used: "llama3-70b-8192"
  })
}
```

---

## 6. Security and Best Practices

### 6.1 API Key Management

```typescript
// Environment variable configuration
// .env.local
GROQ_API_KEY=gsk_your_actual_api_key_here

// Secure usage in API routes
const groqApiKey = process.env.GROQ_API_KEY
if (!groqApiKey) {
  throw new Error("GROQ_API_KEY environment variable is required")
}
```

### 6.2 Input Validation and Sanitization

```typescript
// Validate and sanitize user input before sending to Groq
const validateMessage = (content: string): string => {
  // 1. Length validation
  if (content.length > 4000) {
    throw new Error("Message too long")
  }
  
  // 2. Content filtering
  const sanitized = content.trim()
  if (!sanitized) {
    throw new Error("Message cannot be empty")
  }
  
  // 3. Rate limiting could be added here
  return sanitized
}
```

---

## Summary

The Groq integration in FlowGenie provides:

1. **Ultra-fast AI responses** through specialized hardware
2. **Seamless OpenAI-compatible API** integration
3. **Cost-effective** AI inference compared to alternatives
4. **Real-time chat functionality** with minimal latency
5. **Automatic title generation** for better UX
6. **Comprehensive error handling** and fallback mechanisms


The integration follows modern best practices with proper authentication, database persistence, and user experience optimization, making Groq a powerful engine for the chat application's AI capabilities.

## GROQ USED IN REAL PROJECTS
# How Groq is Integrated in the FlowGenie Project

Let me walk you through the complete Groq integration in this project, showing exactly how it works from setup to execution.

## 🔧 **Integration Architecture Overview**

```mermaid
Diagram.download-icon {
            cursor: pointer;
            transform-origin: center;
        }
        .download-icon .arrow-part {
            transition: transform 0.35s cubic-bezier(0.35, 0.2, 0.14, 0.95);
             transform-origin: center;
        }
        button:has(.download-icon):hover .download-icon .arrow-part, button:has(.download-icon):focus-visible .download-icon .arrow-part {
          transform: translateY(-1.5px);
        }
        #mermaid-diagram-r1tk{font-family:var(--font-geist-sans);font-size:12px;fill:#000000;}#mermaid-diagram-r1tk .error-icon{fill:#552222;}#mermaid-diagram-r1tk .error-text{fill:#552222;stroke:#552222;}#mermaid-diagram-r1tk .edge-thickness-normal{stroke-width:1px;}#mermaid-diagram-r1tk .edge-thickness-thick{stroke-width:3.5px;}#mermaid-diagram-r1tk .edge-pattern-solid{stroke-dasharray:0;}#mermaid-diagram-r1tk .edge-thickness-invisible{stroke-width:0;fill:none;}#mermaid-diagram-r1tk .edge-pattern-dashed{stroke-dasharray:3;}#mermaid-diagram-r1tk .edge-pattern-dotted{stroke-dasharray:2;}#mermaid-diagram-r1tk .marker{fill:#666;stroke:#666;}#mermaid-diagram-r1tk .marker.cross{stroke:#666;}#mermaid-diagram-r1tk svg{font-family:var(--font-geist-sans);font-size:12px;}#mermaid-diagram-r1tk p{margin:0;}#mermaid-diagram-r1tk .label{font-family:var(--font-geist-sans);color:#000000;}#mermaid-diagram-r1tk .cluster-label text{fill:#333;}#mermaid-diagram-r1tk .cluster-label span{color:#333;}#mermaid-diagram-r1tk .cluster-label span p{background-color:transparent;}#mermaid-diagram-r1tk .label text,#mermaid-diagram-r1tk span{fill:#000000;color:#000000;}#mermaid-diagram-r1tk .node rect,#mermaid-diagram-r1tk .node circle,#mermaid-diagram-r1tk .node ellipse,#mermaid-diagram-r1tk .node polygon,#mermaid-diagram-r1tk .node path{fill:#eee;stroke:#999;stroke-width:1px;}#mermaid-diagram-r1tk .rough-node .label text,#mermaid-diagram-r1tk .node .label text{text-anchor:middle;}#mermaid-diagram-r1tk .node .katex path{fill:#000;stroke:#000;stroke-width:1px;}#mermaid-diagram-r1tk .node .label{text-align:center;}#mermaid-diagram-r1tk .node.clickable{cursor:pointer;}#mermaid-diagram-r1tk .arrowheadPath{fill:#333333;}#mermaid-diagram-r1tk .edgePath .path{stroke:#666;stroke-width:2.0px;}#mermaid-diagram-r1tk .flowchart-link{stroke:#666;fill:none;}#mermaid-diagram-r1tk .edgeLabel{background-color:white;text-align:center;}#mermaid-diagram-r1tk .edgeLabel p{background-color:white;}#mermaid-diagram-r1tk .edgeLabel rect{opacity:0.5;background-color:white;fill:white;}#mermaid-diagram-r1tk .labelBkg{background-color:rgba(255, 255, 255, 0.5);}#mermaid-diagram-r1tk .cluster rect{fill:hsl(0, 0%, 98.9215686275%);stroke:#707070;stroke-width:1px;}#mermaid-diagram-r1tk .cluster text{fill:#333;}#mermaid-diagram-r1tk .cluster span{color:#333;}#mermaid-diagram-r1tk div.mermaidTooltip{position:absolute;text-align:center;max-width:200px;padding:2px;font-family:var(--font-geist-sans);font-size:12px;background:hsl(-160, 0%, 93.3333333333%);border:1px solid #707070;border-radius:2px;pointer-events:none;z-index:100;}#mermaid-diagram-r1tk .flowchartTitleText{text-anchor:middle;font-size:18px;fill:#000000;}#mermaid-diagram-r1tk .flowchart-link{stroke:hsl(var(--gray-400));stroke-width:1px;}#mermaid-diagram-r1tk .marker,#mermaid-diagram-r1tk marker,#mermaid-diagram-r1tk marker *{fill:hsl(var(--gray-400))!important;stroke:hsl(var(--gray-400))!important;}#mermaid-diagram-r1tk .label,#mermaid-diagram-r1tk text,#mermaid-diagram-r1tk text>tspan{fill:hsl(var(--black))!important;color:hsl(var(--black))!important;}#mermaid-diagram-r1tk .background,#mermaid-diagram-r1tk rect.relationshipLabelBox{fill:hsl(var(--white))!important;}#mermaid-diagram-r1tk .entityBox,#mermaid-diagram-r1tk .attributeBoxEven{fill:hsl(var(--gray-150))!important;}#mermaid-diagram-r1tk .attributeBoxOdd{fill:hsl(var(--white))!important;}#mermaid-diagram-r1tk .label-container,#mermaid-diagram-r1tk rect.actor{fill:hsl(var(--white))!important;stroke:hsl(var(--gray-400))!important;}#mermaid-diagram-r1tk line{stroke:hsl(var(--gray-400))!important;}#mermaid-diagram-r1tk :root{--mermaid-font-family:var(--font-geist-sans);}User Types MessageChat Input ComponentuseChat HookChatService.addMessageSave User Message to SupabaseCall /api/chat/askGroq API RequestAI ResponseSave AI Message to SupabaseUpdate UI with ResponseAuto-generate Title if FirstMessage
```

## 📁 **File Structure & Components**

### **1. Environment Setup**

```shellscript
# Environment Variables (.env.local)
GROQ_API_KEY=gsk_your_groq_api_key_here
```

### **2. API Routes (Backend Integration)**

**Main Chat API (`app/api/chat/ask/route.ts`):**

```typescript
import { type NextRequest, NextResponse } from "next/server"

export async function POST(req: NextRequest) {
  try {
    // Extract chat parameters from frontend request
    const { messages, temperature = 0.7, model = "llama3-70b-8192" } = await req.json()

    // Direct API call to Groq's OpenAI-compatible endpoint
    const res = await fetch("https://api.groq.com/openai/v1/chat/completions", {
      method: "POST",
      headers: {
        Authorization: `Bearer ${process.env.GROQ_API_KEY}`,
        "Content-Type": "application/json",
      },
      body: JSON.stringify({
        messages,      // Conversation history
        temperature,   // Response creativity (0.0-1.0)
        model,        // Groq model (llama3-70b-8192)
      }),
    })

    if (!res.ok) {
      const errorData = await res.json()
      return NextResponse.json(
        { error: "Failed to get response from Groq API", details: errorData },
        { status: res.status },
      )
    }

    const data = await res.json()
    const content = data?.choices?.[0]?.message?.content

    return NextResponse.json({ content })
  } catch (error) {
    console.error("Error in chat API route:", error)
    return NextResponse.json({ error: "An unexpected error occurred" }, { status: 500 })
  }
}
```

**Title Generation API (`app/api/chat/generate-title/route.ts`):**

```typescript
export async function POST(req: NextRequest) {
  try {
    const { content } = await req.json()

    // Specialized prompt for generating chat titles
    const prompt = `Generate a concise, descriptive title (max 50 characters) for a chat conversation based on this content: "${content.substring(0, 200)}..."`

    const res = await fetch("https://api.groq.com/openai/v1/chat/completions", {
      method: "POST",
      headers: {
        Authorization: `Bearer ${process.env.GROQ_API_KEY}`,
        "Content-Type": "application/json",
      },
      body: JSON.stringify({
        messages: [
          {
            role: "system",
            content: "You are a helpful assistant that generates concise, descriptive titles for chat conversations. Keep titles under 50 characters and make them specific to the content discussed.",
          },
          {
            role: "user",
            content: prompt,
          },
        ],
        temperature: 0.3,  // Lower temperature for consistent titles
        model: "llama3-70b-8192",
        max_tokens: 50,    // Limit response length
      }),
    })

    const data = await res.json()
    const title = data?.choices?.[0]?.message?.content?.trim()
    
    // Clean up the title
    const cleanTitle = title
      ?.replace(/^["']|["']$/g, "") // Remove quotes
      ?.substring(0, 50)            // Limit length
      ?.trim() || "New Chat"

    return NextResponse.json({ title: cleanTitle })
  } catch (error) {
    console.error("Error in generate-title API route:", error)
    return NextResponse.json({ error: "An unexpected error occurred" }, { status: 500 })
  }
}
```

### **3. Frontend Integration (React Hooks)**

**Chat Hook (`hooks/use-chat.ts`):**

```typescript
export function useChat() {
  const { user } = useAuth()
  const [messages, setMessages] = useState<ChatMessage[]>([])
  const [currentThread, setCurrentThread] = useState<ChatThread | null>(null)

  // Main function that integrates with Groq
  const sendMessage = useCallback(
    async (content: string, threadId?: string) => {
      if (!user) return null

      let targetThreadId = threadId || currentThread?.id

      // Create new thread if none exists
      if (!targetThreadId) {
        const newThread = await createThread()
        if (!newThread) return null
        targetThreadId = newThread.id
      }

      try {
        // 1. Add user message to database
        const { data: userMessage, error: userError } = await ChatService.addMessage(
          targetThreadId,
          user.id,
          "user",
          content,
        )

        if (userError) throw userError

        if (userMessage) {
          setMessages((prev) => [...prev, userMessage])

          // 2. Generate AI response using Groq
          const aiResponse = await generateAIResponse(content, targetThreadId)

          if (aiResponse) {
            setMessages((prev) => [...prev, aiResponse])

            // 3. Auto-generate title if this is the first message
            if (messages.length === 0) {
              await ChatService.generateThreadTitle(targetThreadId)
              await loadThreads() // Refresh threads to show new title
            }
          }

          return userMessage
        }
      } catch (err) {
        setError(err instanceof Error ? err.message : "Failed to send message")
      }
      return null
    },
    [user, currentThread, messages.length, createThread, loadThreads],
  )

  // Function that calls Groq API
  const generateAIResponse = async (userMessage: string, threadId: string): Promise<ChatMessage | null> => {
    if (!user) return null

    try {
      // Get conversation history from database
      const { data: history } = await ChatService.getThreadMessages(threadId)

      // Format messages for Groq API
      const messageHistory = [
        {
          role: "system",
          content: `You are FlowGenie, a specialized AI assistant for n8n automation workflows. Help users build, explain, and fix automations with clear, step-by-step guidance.`,
        },
        ...(history || []).map((msg) => ({
          role: msg.role,
          content: msg.content,
        })),
        { role: "user", content: userMessage },
      ]

      const startTime = Date.now()

      // Call our API route which connects to Groq
      const response = await fetch("/api/chat/ask", {
        method: "POST",
        headers: { "Content-Type": "application/json" },
        body: JSON.stringify({
          messages: messageHistory,
          temperature: 0.7,
          model: "llama3-70b-8192",
        }),
      })

      if (!response.ok) throw new Error("Failed to get AI response")

      const data = await response.json()
      const processingTime = Date.now() - startTime

      // Save AI response to database with metadata
      const { data: aiMessage, error } = await ChatService.addMessage(
        threadId,
        user.id,
        "assistant",
        data.content || "Sorry, I couldn't generate a response.",
        {
          model_used: "llama3-70b-8192",
          processing_time_ms: processingTime,
          tokens_used: data.usage?.total_tokens,
        },
      )

      if (error) throw error
      return aiMessage
    } catch (err) {
      console.error("Generate AI response error:", err)

      // Add error message if Groq fails
      const { data: errorMessage } = await ChatService.addMessage(
        threadId,
        user.id,
        "assistant",
        "Sorry, there was an error processing your request. Please try again.",
      )

      return errorMessage
    }
  }

  return {
    messages,
    sendMessage,
    // ... other chat functions
  }
}
```

### **4. Database Integration (ChatService)**

**ChatService with Groq Integration (`lib/chat-service.ts`):**

```typescript
export class ChatService {
  // Function that handles title generation using Groq
  static async generateThreadTitle(threadId: string) {
    try {
      // Get first few user messages for context
      const { data: messages } = await supabase
        .from("chat_messages")
        .select("content")
        .eq("thread_id", threadId)
        .eq("role", "user")
        .order("created_at", { ascending: true })
        .limit(3)

      if (!messages || messages.length === 0) return { data: null, error: "No messages found" }

      // Combine messages for context
      const context = messages.map((m) => m.content).join(" ")

      // Call Groq API for title generation
      const title = await this.generateTitleFromContent(context)

      // Update thread with generated title
      const { data, error } = await this.updateChatThread(threadId, { title })

      if (error) throw error
      return { data, error: null }
    } catch (error) {
      console.error("Generate thread title error:", error)
      return { data: null, error }
    }
  }

  private static async generateTitleFromContent(content: string): Promise<string> {
    try {
      // Call our title generation API (which uses Groq)
      const response = await fetch("/api/chat/generate-title", {
        method: "POST",
        headers: { "Content-Type": "application/json" },
        body: JSON.stringify({ content }),
      })

      if (!response.ok) throw new Error("Failed to generate title")

      const { title } = await response.json()
      return title || "New Chat"
    } catch (error) {
      console.error("Generate title from content error:", error)
      // Fallback: create title from first few words
      const words = content.split(" ").slice(0, 4).join(" ")
      return words.length > 30 ? words.substring(0, 30) + "..." : words || "New Chat"
    }
  }

  // Function that saves messages with Groq metadata
  static async addMessage(
    threadId: string,
    userId: string,
    role: "user" | "assistant",
    content: string,
    metadata?: {
      tokens_used?: number
      model_used?: string
      processing_time_ms?: number
      [key: string]: any
    },
  ) {
    try {
      const { data, error } = await supabase
        .from("chat_messages")
        .insert({
          thread_id: threadId,
          user_id: userId,
          role,
          content,
          metadata: metadata || {},
          tokens_used: metadata?.tokens_used,      // Track Groq token usage
          model_used: metadata?.model_used,        // Track which Groq model was used
          processing_time_ms: metadata?.processing_time_ms, // Track response time
        })
        .select()
        .single()

      if (error) throw error

      // Update thread metadata
      await this.updateThreadMetadata(threadId)

      return { data, error: null }
    } catch (error) {
      console.error("Add message error:", error)
      return { data: null, error }
    }
  }
}
```

## 🔄 **Complete Message Flow**

Here's exactly what happens when a user sends a message:

### **Step 1: User Input**

```typescript
// User types in chat input and presses send
<ChatInput onSend={(message) => sendMessage(message)} />
```

### **Step 2: Frontend Processing**

```typescript
// useChat hook processes the message
const sendMessage = async (content: string) => {
  // Add user message to UI immediately (optimistic update)
  setMessages(prev => [...prev, userMessage])
  
  // Save to database
  await ChatService.addMessage(threadId, userId, "user", content)
  
  // Get AI response
  const aiResponse = await generateAIResponse(content, threadId)
}
```

### **Step 3: Groq API Call**

```typescript
// generateAIResponse function calls Groq
const response = await fetch("/api/chat/ask", {
  method: "POST",
  body: JSON.stringify({
    messages: conversationHistory,
    temperature: 0.7,
    model: "llama3-70b-8192"
  })
})
```

### **Step 4: API Route Processing**

```typescript
// /api/chat/ask route forwards to Groq
const groqResponse = await fetch("https://api.groq.com/openai/v1/chat/completions", {
  headers: { Authorization: `Bearer ${process.env.GROQ_API_KEY}` },
  body: JSON.stringify(groqRequest)
})
```

### **Step 5: Response Processing**

```typescript
// AI response comes back and gets saved
await ChatService.addMessage(threadId, userId, "assistant", aiResponse, {
  model_used: "llama3-70b-8192",
  processing_time_ms: responseTime,
  tokens_used: tokenCount
})
```

### **Step 6: UI Update**

```typescript
// UI updates with AI response
setMessages(prev => [...prev, aiMessage])
```

## 🎯 **Key Integration Points**

### **1. Authentication Integration**

- Groq calls are authenticated using environment variables
- User authentication handled by Supabase
- Each message tied to authenticated user


### **2. Database Integration**

- All messages stored in Supabase with Groq metadata
- Token usage and model information tracked
- Response times recorded for analytics


### **3. Error Handling**

- Multiple fallback layers for Groq API failures
- Graceful degradation when Groq is unavailable
- User-friendly error messages


### **4. Performance Optimization**

- Groq chosen for ultra-fast inference (200-500ms responses)
- Conversation history managed efficiently
- Token usage optimized with context limits


## 📊 **Groq Models Used**

The project uses **`llama3-70b-8192`** as the primary model:

- **70B parameters**: High-quality responses
- **8192 context window**: Good conversation memory
- **Fast inference**: Groq's specialized hardware advantage
- **Cost-effective**: Competitive pricing


## 🔒 **Security Considerations**

1. **API Key Protection**: Stored in environment variables, never exposed to frontend
2. **Input Validation**: User messages validated before sending to Groq
3. **Rate Limiting**: Could be added to prevent API abuse
4. **Content Filtering**: Basic validation on message content


This integration makes Groq the AI brain of FlowGenie, providing fast, intelligent responses while maintaining proper data persistence and user experience.

# MUSTY BAUHAUS BRUTALISM UI RULESET

## DESIGN STYLE — CORE

SET design_theme TO "Bauhaus Brutalism"

APPLY:
- PRIORITIZE function over form
- ENFORCE high-contrast, raw visual hierarchy
- USE bold geometric elements ONLY
- DISALLOW soft edges, gradients, or skeuomorphic styling

---

## STRUCTURE — SYSTEM PRINCIPLES

SET layout_grid TO 12-column
SET content_max_width TO 1200px
SET base_spacing_unit TO 4px

DEFINE spacing_scale:
  xs: 4px
  sm: 8px
  md: 16px
  lg: 24px
  xl: 32px
  2xl: 48px

DEFINE breakpoints:
  mobile: 0–767px
  tablet: 768–1023px
  desktop: 1024px+

ENFORCE responsive-first styling

---

## COLORS — PALETTE

SET primary_red TO #dc2626
SET primary_blue TO #2563eb
SET accent_yellow TO #eab308
SET black TO #000000
SET white TO #ffffff

USE:
- Red: HEADINGS, CRITICAL ACTIONS
- Blue: INTERACTIVES, BUTTONS
- Yellow: HIGHLIGHT, CTA
- Black: BORDERS, TEXT
- White: BACKGROUND

MAINTAIN minimum contrast ratio: WCAG AA+

---

## TYPOGRAPHY

USE:
- PRIMARY_FONT: Inter, sans-serif
- DISPLAY_FONT: System Mono

STYLE:
- HEADINGS: uppercase, font-black (900), tight letter-spacing (-0.025em)
- BODY: font-bold (700) or font-normal (400)
- LINE_HEIGHT: 1.5–1.6

MAKE all interactive text bold
ADD scale-105 on hover for clickable text

SCALE type across breakpoints:
- Mobile: base scale
- Tablet: scale 1.25×
- Desktop: scale 1.5× headers

---

## COMPONENTS — CARDS

BASE_CARD:
- bg-white
- border: 8px solid black
- box-shadow: 8px offset (shadow-brutal)
- text-align: left

ON hover:
- translateY(-4px)
- REMOVE shadow

VARIANTS:
- PRIMARY: bg-blue, text-white
- SECONDARY: bg-yellow, text-black
- DANGER: bg-red, text-white
- SUCCESS: bg-green, text-white
- DARK: bg-black, text-white, white-shadow

APPLY uniform border-radius: none

---

## COMPONENTS — BUTTONS

BASE_BUTTON:
- text-uppercase
- border: 4px solid black
- font-weight: black (900)
- shadow-brutal
- size: h-12 (default)

VARIANTS:
- Default: bg-white text-black
- Primary: bg-blue text-white
- Secondary: bg-yellow text-black
- Destructive: bg-red text-white
- Success: bg-green text-white

ON hover:
- translateY(4px)
- REMOVE shadow
- APPLY color-shift (duration: 150ms)

---

## COMPONENTS — FORMS

INPUTS:
- border: 4px solid black
- no border-radius
- font-bold labels
- large padding
- contrast-focused outline on focus

USE grid for forms. 1fr columns on mobile, 2–3fr on tablet+.

---

## INTERACTIONS

ON hover:
- cards: lift + shadow disappear
- buttons: press down + shadow disappear
- icon-button: scale-110 or translateX(8px)

RESPECT prefers-reduced-motion

LIMIT all animations to 300ms max
AVOID autoplay or looped motion

---

## PAGE RESPONSIVENESS

CARDS:
- stack vertically on mobile
- use `grid-cols-1 sm:grid-cols-2 md:grid-cols-3` layout

NAVIGATION:
- horizontal on desktop
- collapsible menu on mobile
- highlight active items with `border-l-4`

---

## LAYERING — VISUAL DEPTH

z-index hierarchy:
1. Backgrounds (z-0)
2. Content Cards (z-10)
3. Navbars (z-20)
4. Modals/Alerts (z-50)

---

## ACCESSIBILITY RULES

ENSURE:
- All text contrast ≥ 7:1
- Focus outlines visible and bold
- Tab order logical
- Add skip links
- Screen-reader roles defined

---

## BRAND SYSTEM — MASCOT

USE mascot as:
- Positioned CTA guide
- Geometric bounce: `animate-bounce-gentle`
- Brutalist speech bubble
- Never use soft gradients or curves

---

## CONSISTENCY ENFORCEMENT

MANDATE:
- Uniform shadow system
- Grid-aligned layout
- No rounded borders
- Consistent use of font weights
- Color usage per design tokens only

---

APPLY these rules globally across all views and modules within MUSTY.
FAILURE TO COMPLY triggers layout audit or rejection.

# MUSTY – Page-Level MVP & PRD (Updated)

---

## 📦 Updated Planning & Workflow Process

1. **Define Scope per Page**: Dashboard, Syllabus, Schedule, Resources, StudGem AI, Analytics, Flashcards, Mindmap, Peer Notes
2. **Identify User Goals for Each Page**: What students need to learn, revise, access, or use AI for.
3. **List MVP Components**: For each page, specify UI elements and logic that meet basic user needs.
4. **Draft PRD Components**: Functional and non-functional specs per page.
5. **Gamification, Reviews, and Open Access**: NO login required. Users can access everything. Progress stored locally.
6. **Review and Iterate**: Keep adding more student-facing features without overloading them.

---

## 1. Dashboard Page

**User Goals:** See progress, upcoming exams, access quick links to features.

### MVP Components

* Gamified Cards: Syllabus, Schedule, Resources, StudGem AI, Flashcards, Mindmap, Quiz, Peer Notes
* XP/progress rings (stored locally)
* Notification area (general MU alerts)
* Year, Semester, Branch, Electives selector (sticky or top-bar)
* Selections are globally saved and applied to all modules

### PRD

* No login needed
* Clicking a card → go to corresponding module using already selected academic context
* All progress saved to `localStorage`
* Cards use Bauhaus brutalist style: thick outlines, bold colors, grid layout
* Each card (like Flashcards, Mindmap, Quiz) opens StudGem AI page with the correct mode pre-selected and context pre-filled

---

## 2. Syllabus Page

**User Goals:** View and download official MU syllabus PDFs based on dashboard selection

### MVP Components

* Subject cards auto-filtered using dashboard context (Year, Semester)
* Each subject card: Subject name, Course Code, Download Syllabus button

### PRD

* No selection inputs on this page (uses existing dashboard state)
* Only official PDF downloads shown — no breakdown of modules/topics
* Backend still uses subject context for AI if needed (e.g., in StudGem AI)
* Fast and minimal: 1-click access to syllabus
* Used by backend to feed StudGem AI when "syllabus" mode is selected

---

## 3. Schedule Page

**User Goals:** See MU exam dates + create custom study sessions

### MVP Components

* MU Timetable (auto updated)
* User-defined events (stored locally)
* Calendar view + countdowns

### PRD

* MU dates auto-fetched or uploaded manually
* "Add event" feature to plan personal deadlines
* Sync to Google Calendar (optional)

---

## 4. Resources Page

**User Goals:** Browse previous year papers, question banks, answers, and shared notes

### MVP Components

* Tabs: PYQs / PYQ Answers / Question Banks / Timetable / Peer Notes
* Filter auto-applies based on selected year/semester from dashboard
* Cards with download/view buttons
* Bookmark option

### PRD

* No login needed
* Peer Notes: upload form with tagging (year, sem, subject, type)
* Cards can be rated with 👍 or ⭐⭐⭐⭐

---

## 5. StudGem AI Page

**User Goals:** Ask AI for help using syllabus context or uploaded notes, generate flashcards, mindmaps, quizzes from same base

### MVP Components

* On first use: Ask user if they want to continue with existing dashboard selections or switch context
* If "Generate from Syllabus" → system auto-loads syllabus file using dashboard's year, semester, subject (if selected)
* If "Upload Notes" → user adds PDFs/typed content
* AI Chat interface
* Flashcards Mode
* Mindmap Mode
* Quiz Mode
* Tab/card-style UI to switch between modes

### PRD

* Global context passed from dashboard unless overridden
* All AI actions (chat/quiz/mindmap/flashcard) reuse the same selected source: syllabus or notes
* Context switcher allows user to reselect anytime
* No login; session stored in browser

---

## 6. Flashcards Mode (inside StudGem AI)

**User Goals:** Learn using generated flashcards (from notes, syllabus, AI answers)

### MVP Components

* Flashcard viewer: front/back
* Shuffle, Flip, Next, Previous buttons
* Mark as "known" or "review again"

### PRD

* Uses dashboard context + AI source type
* Session local only
* Share/export option optional

---

## 7. Mindmap Mode (inside StudGem AI)

**User Goals:** Visualize a topic using an AI-generated mindmap

### MVP Components

* Bubble/tree layout of subject → module → topic
* Clickable bubbles to explore
* Export or print option

### PRD

* Follows same data flow as other StudGem AI modes
* Swappable to flashcard/quiz/chat modes

---

## 8. Quiz Mode (inside StudGem AI)

**User Goals:** Practice and test understanding of syllabus or uploaded content

### MVP Components

* Multiple choice or short-answer quiz
* Instant feedback with explanation
* Quiz generator: choose chapter or full subject

### PRD

* Inherits source context (syllabus or uploaded)
* Locally stored scores
* Retry button

---

## 9. Analytics Page

**User Goals:** See what’s done, what’s pending, what’s likely to appear in exams

### MVP Components

* Progress bars per subject
* Heatmap calendar for sessions
* Prediction cards (e.g., “High chance topics based on PYQs”)

### PRD

* Context-dependent analytics (based on selected year/sem/subject)
* No login required
* All data stored in localStorage

---

## 10. Peer Notes Page

**User Goals:** Explore and upload student-created notes by tag

### MVP Components

* Upload form with metadata
* Filterable gallery
* Rating/Report buttons

### PRD

* Context-based filtering from dashboard
* Public and moderated
* Optional tagging for AI use
* Rate with "👍", "⭐" or "Was this helpful?"

---

## 11. Reviews/Rating System (Global)

**User Goals:** Trust shared notes, improve AI

### MVP Components

* Rate: 👍 / 👎 or ⭐ 1–5 on any resource/AI reply
* Track upvotes/downvotes

### PRD

* Store count + highlight popular notes
* Use feedback to improve AI logic (optional)

---


